# yazzzam

YazzZam is a cutting-edge, easy-to-use search engine for songs. It allows users to type in lyrics for a song and retrieve all relevant songs. Additionally, it allows users to filter by artist, genre, or release year. YazzZam is also planned to have audio search which allows a user to record a part of a song that is playing.

Song Data: In this project we plan to use creative commons licensed music and audio for our models. This type of license allows us to use songs and music from several artists without any copyright issues. We plan to use APIs such as freesound , Openverse and Fugue to download a database of songs from different genres for training our models. From this database we plan to only store the fingerprint of the songs generated by our model on the cloud machine for maximizing our efficiency. 

Queries: The most applicable forms of querying for our search engine would likely be proximity search and phrase search since it is important for our engine to return the songs that contain the words in the order specified by the user. Since the user will be querying the engine mainly using the song lyrics, it is important to take into account the order in which the words occur in the lyrics so that accurate results can be retrieved. An extension to this would be to use Boolean search to filter the metadata such as artist name, album name, genre and year of release.

Indexing: Data indexing is at the core of each search engine. For the textual song search we plan to index song’s title and lyrics together with accompanying metadata. The songs’ metadata would be a useful addition for forming more complex queries and narrowing down the search results. We consider various approaches for tokenization of lyrics that optimize for phrase searches allowing misspelled queries while maintaining the ordering information from the lyrics. For audio search we plan to index songs’ fingerprints in a efficient data structure that allows fast comparisons between fingerprints

Audio search: In addition to the core text search functionalities, we plan on implementing Audio Search feature. A user will hit a button to record a part of a song that is playing and then we use signal processing techniques to fingerprint a song and check our database for similarities.

Audio search algorithm: After producing a spectrogram from the query (music) we can then filter this spectrogram and quantize it to help filter out noise, this produces the fingerprint. 
To match our query with the fingerprints in the index. We look for several notes that exist in our index and are separated by the same separation time in our query.
